{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waleed_daud_wd/CondaEnvs/opensimEnv_V2/lib/python3.6/site-packages/gym/__init__.py:15: UserWarning: gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\n",
      "  warnings.warn(\"gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()  # NOQA\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import gym\n",
    "gym.undo_logger_setup()  # NOQA\n",
    "from gym import spaces\n",
    "import gym.wrappers\n",
    "\n",
    "\n",
    "import chainer\n",
    "from chainer import optimizers\n",
    "from chainerrl.agents.ddpg import DDPG\n",
    "from chainerrl.agents.ddpg import DDPGModel\n",
    "from chainerrl import experiments\n",
    "from chainerrl import explorers\n",
    "from chainerrl import misc\n",
    "from chainerrl import policy\n",
    "from chainerrl import q_functions\n",
    "from chainerrl import replay_buffer\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment settings\n",
    "\n",
    "env_name='BipedalWalker-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chainer's settings\n",
    "seed=0\n",
    "gpu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Setting\n",
    "\n",
    "actor_hidden_layers=3\n",
    "actor_hidden_units=300\n",
    "actor_lr=1e-4\n",
    "\n",
    "\n",
    "critic_hidden_layers=3\n",
    "critic_hidden_units=300\n",
    "critic_lr=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other settings\n",
    "\n",
    "number_of_episodes=10000\n",
    "max_episode_length=500\n",
    "\n",
    "replay_buffer_size=5 * 10 ** 5\n",
    "replay_start_size=50000\n",
    "number_of_update_times=1\n",
    "\n",
    "target_update_interval=1\n",
    "target_update_method='soft'\n",
    "\n",
    "soft_update_tau=1e-2\n",
    "update_interval=4\n",
    "number_of_eval_runs=100\n",
    "eval_interval=10 ** 5\n",
    "\n",
    "final_exploration_steps=10 ** 6\n",
    "\n",
    "gamma=0.995\n",
    "minibatch_size=128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper's functions\n",
    "\n",
    "def clip_action_filter(a):\n",
    "    return np.clip(a, action_space.low, action_space.high)\n",
    "\n",
    "def reward_filter(r):        # reward scale is: 1\n",
    "    return r * 1\n",
    "\n",
    "\n",
    "def phi(obs):\n",
    "    return obs.astype(np.float32)\n",
    "\n",
    "def random_action():\n",
    "    a = action_space.sample()\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = a.astype(np.float32)\n",
    "    return a\n",
    "\n",
    "\n",
    "def make_env(test,env_name,render=False):\n",
    "    env = gym.make(env_name)\n",
    "    # Use different random seeds for train and test envs\n",
    "    env_seed = 2 ** 32 - 1 - seed if test else seed\n",
    "    env.seed(env_seed)\n",
    "    #if args.monitor:\n",
    "        #env = gym.wrappers.Monitor(env, args.outdir)\n",
    "    if isinstance(env.action_space, spaces.Box):\n",
    "        misc.env_modifiers.make_action_filtered(env, clip_action_filter)\n",
    "    if not test:\n",
    "        misc.env_modifiers.make_reward_filtered(env, reward_filter)\n",
    "    if render and not test:\n",
    "        misc.env_modifiers.make_rendered(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed used in ChainerRL\n",
    "misc.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = make_env(test=False,env_name=env_name,render=False)\n",
    "timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "obs_size = np.asarray(env.observation_space.shape).prod()\n",
    "action_space = env.action_space\n",
    "\n",
    "action_size = np.asarray(action_space.shape).prod()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "\n",
    "q_func = q_functions.FCSAQFunction(\n",
    "            obs_size, \n",
    "            action_size,\n",
    "            n_hidden_channels=critic_hidden_units,\n",
    "            n_hidden_layers=critic_hidden_layers)\n",
    "\n",
    "# policy Network\n",
    "\n",
    "pi = policy.FCDeterministicPolicy(\n",
    "            obs_size, \n",
    "            action_size=action_size,\n",
    "            n_hidden_channels=actor_hidden_units,\n",
    "            n_hidden_layers=actor_hidden_layers,\n",
    "            min_action=action_space.low, \n",
    "            max_action=action_space.high,\n",
    "            bound_action=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model\n",
    "\n",
    "model = DDPGModel(q_func=q_func, policy=pi)\n",
    "opt_actor = optimizers.Adam(alpha=actor_lr)\n",
    "opt_critic = optimizers.Adam(alpha=critic_lr)\n",
    "opt_actor.setup(model['policy'])\n",
    "opt_critic.setup(model['q_function'])\n",
    "opt_actor.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_a')\n",
    "opt_critic.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_c')\n",
    "\n",
    "rbuf = replay_buffer.ReplayBuffer(replay_buffer_size)\n",
    "ou_sigma = (action_space.high - action_space.low) * 0.2\n",
    "\n",
    "explorer = explorers.AdditiveOU(sigma=ou_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent\n",
    "agent = DDPG(model, opt_actor, opt_critic, rbuf, gamma=gamma,\n",
    "                 explorer=explorer, replay_start_size=replay_start_size,\n",
    "                 target_update_method=target_update_method,\n",
    "                 target_update_interval=target_update_interval,\n",
    "                 update_interval=update_interval,\n",
    "                 soft_update_tau=soft_update_tau,\n",
    "                 n_times_update=number_of_update_times,\n",
    "                 phi=phi,minibatch_size=minibatch_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  10\n",
      "Rewards:  -115.8567079978995\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -104.4450365851455\n",
      "==========================================\n",
      "Episode:  20\n",
      "Rewards:  -116.30052945398725\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -108.72524299432676\n",
      "==========================================\n",
      "Episode:  30\n",
      "Rewards:  -117.26406881289184\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -111.13985947027953\n",
      "==========================================\n",
      "Episode:  40\n",
      "Rewards:  -114.3030295380832\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -111.99672982898147\n",
      "==========================================\n",
      "Episode:  50\n",
      "Rewards:  -139.77679149293527\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -113.16340688603006\n",
      "==========================================\n",
      "Episode:  60\n",
      "Rewards:  -115.6242822906077\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -113.36392853880514\n",
      "==========================================\n",
      "Episode:  70\n",
      "Rewards:  -113.420904454402\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -112.97048228973958\n",
      "==========================================\n",
      "Episode:  80\n",
      "Rewards:  -116.60494609495564\n",
      "Max reward so far:  -97.26116805853943\n",
      "Mean Reward -113.19439347432544\n",
      "==========================================\n",
      "Episode:  90\n",
      "Rewards:  -115.45811605728605\n",
      "Max reward so far:  -97.26116805853943\n",
      "Mean Reward -113.15817838680199\n",
      "==========================================\n",
      "Episode:  100\n",
      "Rewards:  -31.33259664347088\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -112.69039758448743\n",
      "==========================================\n",
      "Episode:  110\n",
      "Rewards:  -124.20055276906987\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.00757177225934\n",
      "==========================================\n",
      "Episode:  120\n",
      "Rewards:  -126.77140345291296\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.44563009988809\n",
      "==========================================\n",
      "Episode:  130\n",
      "Rewards:  -114.5867968087159\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.43771332776475\n",
      "==========================================\n",
      "Episode:  140\n",
      "Rewards:  -119.68853673172929\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.52291632956934\n",
      "==========================================\n",
      "Episode:  150\n",
      "Rewards:  -118.30809609614872\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.68082817291554\n",
      "==========================================\n",
      "Episode:  160\n",
      "Rewards:  -111.3014928765508\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.64255277496284\n",
      "==========================================\n",
      "Episode:  170\n",
      "Rewards:  -118.09837156570889\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.67731673347278\n",
      "==========================================\n",
      "Episode:  180\n",
      "Rewards:  -106.2670202253914\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.63446523873596\n",
      "==========================================\n",
      "Episode:  190\n",
      "Rewards:  -115.90770251683084\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.91636593149138\n",
      "==========================================\n",
      "Episode:  200\n",
      "Rewards:  -114.08354447274283\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.00198070796793\n",
      "==========================================\n",
      "Episode:  210\n",
      "Rewards:  -111.19713517421981\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.88271119683172\n",
      "==========================================\n",
      "Episode:  220\n",
      "Rewards:  -132.68465238174548\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.99908932051271\n",
      "==========================================\n",
      "Episode:  230\n",
      "Rewards:  -105.44085757796032\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.09131625236687\n",
      "==========================================\n",
      "Episode:  240\n",
      "Rewards:  -132.21310814891694\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.15797261911824\n",
      "==========================================\n",
      "Episode:  250\n",
      "Rewards:  -127.11885565675422\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.26901652663558\n",
      "==========================================\n",
      "Episode:  260\n",
      "Rewards:  -118.22869849932752\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.38443617151731\n",
      "==========================================\n",
      "Episode:  270\n",
      "Rewards:  -112.65967043116751\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.69056191808959\n",
      "==========================================\n",
      "Episode:  280\n",
      "Rewards:  -114.20493240654655\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.72276340691963\n",
      "==========================================\n",
      "Episode:  290\n",
      "Rewards:  -127.38607375766088\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.82221909867481\n",
      "==========================================\n",
      "Episode:  300\n",
      "Rewards:  -117.74083410133919\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.67289704037492\n",
      "==========================================\n",
      "Episode:  310\n",
      "Rewards:  -119.96448366764933\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.62142055983207\n",
      "==========================================\n",
      "Episode:  320\n",
      "Rewards:  -116.67750092816415\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.6436162107747\n",
      "==========================================\n",
      "Episode:  330\n",
      "Rewards:  -120.67376588768884\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.79425642142937\n",
      "==========================================\n",
      "Episode:  340\n",
      "Rewards:  -114.94637148646886\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.76543875465323\n",
      "==========================================\n",
      "Episode:  350\n",
      "Rewards:  -111.31504513684226\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.57033909268509\n",
      "==========================================\n",
      "Episode:  360\n",
      "Rewards:  -125.83114784578794\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.64099521229093\n",
      "==========================================\n",
      "Episode:  370\n",
      "Rewards:  -122.17215255922949\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.67376712881541\n",
      "==========================================\n",
      "Episode:  380\n",
      "Rewards:  -119.5480675100175\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.56931018671739\n",
      "==========================================\n",
      "Episode:  390\n",
      "Rewards:  -110.85851889333439\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.61462112677232\n",
      "==========================================\n",
      "Episode:  400\n",
      "Rewards:  -108.6116478331089\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.65533990071202\n",
      "==========================================\n",
      "Episode:  410\n",
      "Rewards:  -121.72390101982032\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.6590667251667\n",
      "==========================================\n",
      "Episode:  420\n",
      "Rewards:  -111.77414213861587\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.55712367143578\n",
      "==========================================\n",
      "Episode:  430\n",
      "Rewards:  -115.34675224344122\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.47833865668751\n",
      "==========================================\n",
      "Episode:  440\n",
      "Rewards:  -111.01645477890285\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.25607821924116\n"
     ]
    }
   ],
   "source": [
    "G=[]\n",
    "G_mean=[]\n",
    "for ep in range(1, number_of_episodes+ 1):\n",
    "    if ep%100:\n",
    "        agent.save(\"DDPG_Walker2D_10000\")\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    episode_rewards=[]\n",
    "    while not done and t < max_episode_length:\n",
    "        # Uncomment to watch the behaviour\n",
    "        #env.render()\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        episode_rewards.append(reward)\n",
    "        t += 1\n",
    "        \n",
    "    if done or t >= max_episode_length :\n",
    "            \n",
    "            # Calculate sum of the rewards\n",
    "        episode_rewards_sum = sum(episode_rewards)     \n",
    "        G.append(episode_rewards_sum)\n",
    "        total_G = np.sum(G)\n",
    "        maximumReturn = np.amax(G)\n",
    "        print(\"%f\" % (episode_rewards_sum), file=open(\"DDPG_Walker2D_reward_10000.txt\", \"a\"))\n",
    "        if ep % 10 == 0:\n",
    "                \n",
    "            print(\"==========================================\")\n",
    "            print(\"Episode: \", ep)\n",
    "            print(\"Rewards: \", episode_rewards_sum)\n",
    "            print(\"Max reward so far: \", maximumReturn)\n",
    "            # Mean reward\n",
    "            total_reward_mean = np.divide(total_G, ep+1)\n",
    "            G_mean.append(total_reward_mean)\n",
    "            print(\"Mean Reward\", total_reward_mean)\n",
    "            print(\"%f\" % (total_reward_mean), file=open(\"DDPG_Walker2D_MEAN_Reward_10000.txt\", \"a\"))    \n",
    "                \n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    \n",
    "    \n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"DDPG_BiPedalWalker_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G)\n",
    "plt.ylabel('Returns')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.savefig(\"Returns_VS_Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G_mean)\n",
    "plt.ylabel('Average of Returns ')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.savefig(\"ReturnsAverage_VS_Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (OpenSimEnv_V2)",
   "language": "python",
   "name": "opensimenv_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
