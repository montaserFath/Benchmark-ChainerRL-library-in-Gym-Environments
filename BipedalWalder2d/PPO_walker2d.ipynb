{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waleed_daud_wd/CondaEnvs/opensimEnv_V2/lib/python3.6/site-packages/gym/__init__.py:15: UserWarning: gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\n",
      "  warnings.warn(\"gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import absolute_import\n",
    "from builtins import *  # NOQA\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()  # NOQA\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import chainer\n",
    "from chainer import functions as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import gym\n",
    "gym.undo_logger_setup()  # NOQA\n",
    "import gym.wrappers\n",
    "\n",
    "\n",
    "from chainerrl.agents import a3c\n",
    "from chainerrl.agents import PPO\n",
    "from chainerrl import experiments\n",
    "from chainerrl import links\n",
    "from chainerrl import misc\n",
    "from chainerrl.optimizers.nonbias_weight_decay import NonbiasWeightDecay\n",
    "from chainerrl import policies\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment settings\n",
    "\n",
    "#env_name='BipedalWalker-v2'\n",
    "env_name='BipedalWalker-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chainer's settings\n",
    "seed=0\n",
    "gpu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Setting\n",
    "\n",
    "#actor_hidden_layers=3\n",
    "#actor_hidden_units=300\n",
    "actor_lr=1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other settings\n",
    "\n",
    "number_of_episodes=10000\n",
    "max_episode_length=500\n",
    "\n",
    "update_interval=4\n",
    "\n",
    "number_of_eval_runs=100\n",
    "eval_interval=10 ** 5\n",
    "\n",
    "epochs=10\n",
    "gamma=0.995\n",
    "batch_size=128\n",
    "entropy_coef=0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers classes\n",
    "\n",
    "\n",
    "class A3CFFSoftmax(chainer.ChainList, a3c.A3CModel):\n",
    "    \"\"\"An example of A3C feedforward softmax policy.\"\"\"\n",
    "\n",
    "    def __init__(self, ndim_obs, n_actions, hidden_sizes=(200, 200)):\n",
    "        self.pi = policies.SoftmaxPolicy(\n",
    "            model=links.MLP(ndim_obs, n_actions, hidden_sizes))\n",
    "        self.v = links.MLP(ndim_obs, 1, hidden_sizes=hidden_sizes)\n",
    "        super().__init__(self.pi, self.v)\n",
    "\n",
    "    def pi_and_v(self, state):\n",
    "        return self.pi(state), self.v(state)\n",
    "\n",
    "\n",
    "class A3CFFMellowmax(chainer.ChainList, a3c.A3CModel):\n",
    "    \"\"\"An example of A3C feedforward mellowmax policy.\"\"\"\n",
    "\n",
    "    def __init__(self, ndim_obs, n_actions, hidden_sizes=(200, 200)):\n",
    "        self.pi = policies.MellowmaxPolicy(\n",
    "            model=links.MLP(ndim_obs, n_actions, hidden_sizes))\n",
    "        self.v = links.MLP(ndim_obs, 1, hidden_sizes=hidden_sizes)\n",
    "        super().__init__(self.pi, self.v)\n",
    "\n",
    "    def pi_and_v(self, state):\n",
    "        return self.pi(state), self.v(state)\n",
    "\n",
    "\n",
    "class A3CFFGaussian(chainer.Chain, a3c.A3CModel):\n",
    "    \"\"\"An example of A3C feedforward Gaussian policy.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_size, action_space,\n",
    "                 n_hidden_layers=2, n_hidden_channels=64,\n",
    "                 bound_mean=None, normalize_obs=None):\n",
    "        assert bound_mean in [False, True]\n",
    "        assert normalize_obs in [False, True]\n",
    "        super().__init__()\n",
    "        hidden_sizes = (n_hidden_channels,) * n_hidden_layers\n",
    "        self.normalize_obs = normalize_obs\n",
    "        with self.init_scope():\n",
    "            self.pi = policies.FCGaussianPolicyWithStateIndependentCovariance(\n",
    "                obs_size, action_space.low.size,\n",
    "                n_hidden_layers, n_hidden_channels,\n",
    "                var_type='diagonal', nonlinearity=F.tanh,\n",
    "                bound_mean=bound_mean,\n",
    "                min_action=action_space.low, max_action=action_space.high,\n",
    "                mean_wscale=1e-2)\n",
    "            self.v = links.MLP(obs_size, 1, hidden_sizes=hidden_sizes)\n",
    "            if self.normalize_obs:\n",
    "                self.obs_filter = links.EmpiricalNormalization(\n",
    "                    shape=obs_size\n",
    "                )\n",
    "\n",
    "    def pi_and_v(self, state):\n",
    "        if self.normalize_obs:\n",
    "            state = F.clip(self.obs_filter(state, update=False),\n",
    "                           -5.0, 5.0)\n",
    "\n",
    "        return self.pi(state), self.v(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper's functions\n",
    "\n",
    "# Linearly decay the learning rate to zero\n",
    "def lr_setter(env, agent, value):\n",
    "    agent.optimizer.alpha = value\n",
    "\n",
    "# Linearly decay the clipping parameter to zero\n",
    "def clip_eps_setter(env, agent, value):\n",
    "    agent.clip_eps = value\n",
    "\n",
    "\n",
    "def clip_action_filter(a):\n",
    "    return np.clip(a, action_space.low, action_space.high)\n",
    "\n",
    "def reward_filter(r):\n",
    "    return r\n",
    "\n",
    "\n",
    "def phi(obs):\n",
    "    return obs.astype(np.float32)\n",
    "\n",
    "\n",
    "def make_env(test,env_name,render=False):\n",
    "    env = gym.make(env_name)\n",
    "    # Use different random seeds for train and test envs\n",
    "    env_seed = 2 ** 32 - 1 - seed if test else seed\n",
    "    env.seed(env_seed)\n",
    "\n",
    "    if not test:\n",
    "        misc.env_modifiers.make_reward_filtered(env, reward_filter)\n",
    "    if render and not test:\n",
    "        misc.env_modifiers.make_rendered(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed used in ChainerRL\n",
    "misc.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = make_env(test=False,env_name=env_name,render=False)\n",
    "timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "obs_space=env.observation_space\n",
    "obs_size = obs_space.low.size\n",
    "action_space = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A3CFFGaussian(obs_size, action_space,bound_mean=True,normalize_obs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x7fce41a1fe10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = chainer.optimizers.Adam(alpha=actor_lr, eps=1e-5)\n",
    "opt.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(model, opt,\n",
    "                #gpu=args.gpu,\n",
    "                phi=phi,\n",
    "                update_interval=update_interval,\n",
    "                minibatch_size=batch_size, epochs=epochs,\n",
    "                clip_eps_vf=None, entropy_coef=entropy_coef,\n",
    "                #standardize_advantages=args.standardize_advantages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#lr_decay_hook = experiments.LinearInterpolationHook(number_of_steps, actor_lr, 0, lr_setter)\n",
    "\n",
    "#clip_eps_decay_hook = experiments.LinearInterpolationHook(number_of_steps, 0.2, 0, clip_eps_setter)\n",
    "\n",
    "eval_env = make_env(test=True,env_name=env_name,render=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  10\n",
      "Rewards:  -117.70632379305829\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -81.13787097741555\n",
      "==========================================\n",
      "Episode:  20\n",
      "Rewards:  -114.36694514458316\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -83.87680118247103\n",
      "==========================================\n",
      "Episode:  30\n",
      "Rewards:  -113.9336546278422\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -90.21931010762326\n",
      "==========================================\n",
      "Episode:  40\n",
      "Rewards:  -110.93358352814553\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -87.12564274491486\n",
      "==========================================\n",
      "Episode:  50\n",
      "Rewards:  -123.1697991994694\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -88.57354013565165\n",
      "==========================================\n",
      "Episode:  60\n",
      "Rewards:  -25.21445309647794\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -87.64848655490341\n",
      "==========================================\n",
      "Episode:  70\n",
      "Rewards:  -29.175436293069872\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -83.84059099609584\n",
      "==========================================\n",
      "Episode:  80\n",
      "Rewards:  -102.63027975025277\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -81.16332547419266\n",
      "==========================================\n",
      "Episode:  90\n",
      "Rewards:  -100.1293609425916\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -81.43365323160498\n",
      "==========================================\n",
      "Episode:  100\n",
      "Rewards:  -28.867943759805218\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -77.1696451619728\n",
      "==========================================\n",
      "Episode:  110\n",
      "Rewards:  -32.65005028235664\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -75.11697188589483\n",
      "==========================================\n",
      "Episode:  120\n",
      "Rewards:  -113.69295324347914\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -73.3589718761847\n",
      "==========================================\n",
      "Episode:  130\n",
      "Rewards:  -36.98577069156772\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -70.70552066774502\n",
      "==========================================\n",
      "Episode:  140\n",
      "Rewards:  -22.956966056684028\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -67.71408044091437\n",
      "==========================================\n",
      "Episode:  150\n",
      "Rewards:  -26.38376656207753\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -66.2725208377751\n",
      "==========================================\n",
      "Episode:  160\n",
      "Rewards:  -38.496452907618156\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -64.3104601484713\n",
      "==========================================\n",
      "Episode:  170\n",
      "Rewards:  -28.193234061327455\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -62.76855992567325\n",
      "==========================================\n",
      "Episode:  180\n",
      "Rewards:  -37.29669313440037\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -61.15173409401571\n",
      "==========================================\n",
      "Episode:  190\n",
      "Rewards:  -27.753441190782308\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -59.56935829133218\n",
      "==========================================\n",
      "Episode:  200\n",
      "Rewards:  -30.54773700316065\n",
      "Max reward so far:  -20.88962384359461\n",
      "Mean Reward -58.093849175913626\n",
      "==========================================\n",
      "Episode:  210\n",
      "Rewards:  -23.854489773812386\n",
      "Max reward so far:  -20.56818691545105\n",
      "Mean Reward -56.727587622557415\n",
      "==========================================\n",
      "Episode:  220\n",
      "Rewards:  -25.377380662078657\n",
      "Max reward so far:  -20.56818691545105\n",
      "Mean Reward -55.436871334477196\n",
      "==========================================\n",
      "Episode:  230\n",
      "Rewards:  -29.314270918566105\n",
      "Max reward so far:  -19.78430198216936\n",
      "Mean Reward -54.202334937163165\n",
      "==========================================\n",
      "Episode:  240\n",
      "Rewards:  -21.25273452916681\n",
      "Max reward so far:  -15.12900656852809\n",
      "Mean Reward -52.79128344592961\n",
      "==========================================\n",
      "Episode:  250\n",
      "Rewards:  -16.57508784517707\n",
      "Max reward so far:  -11.014002234470722\n",
      "Mean Reward -51.668550774137245\n"
     ]
    }
   ],
   "source": [
    "G=[]\n",
    "G_mean=[]\n",
    "for ep in range(1, number_of_episodes+ 1):\n",
    "    if ep%100:\n",
    "        agent.save(\"PPO_Walker2D_10000\")\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    episode_rewards=[]\n",
    "    while not done and t < max_episode_length:\n",
    "        # Uncomment to watch the behaviour\n",
    "        #env.render()\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        episode_rewards.append(reward)\n",
    "        t += 1\n",
    "        \n",
    "    if done or t >= max_episode_length :\n",
    "            \n",
    "            # Calculate sum of the rewards\n",
    "        episode_rewards_sum = sum(episode_rewards)     \n",
    "        G.append(episode_rewards_sum)\n",
    "        total_G = np.sum(G)\n",
    "        maximumReturn = np.amax(G)\n",
    "        print(\"%f\" % (episode_rewards_sum), file=open(\"PPO_Walker2D_reward_10000.txt\", \"a\"))\n",
    "        if ep % 10 == 0:\n",
    "                \n",
    "            print(\"==========================================\")\n",
    "            print(\"Episode: \", ep)\n",
    "            print(\"Rewards: \", episode_rewards_sum)\n",
    "            print(\"Max reward so far: \", maximumReturn)\n",
    "            # Mean reward\n",
    "            total_reward_mean = np.divide(total_G, ep+1)\n",
    "            G_mean.append(total_reward_mean)\n",
    "            print(\"Mean Reward\", total_reward_mean)\n",
    "            print(\"%f\" % (total_reward_mean), file=open(\"PPO_Walker2D_MEAN_Reward_10000.txt\", \"a\"))   \n",
    "                \n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    \n",
    "    \n",
    "print('Finished.')\n",
    "\n",
    "\n",
    "plt.xlabel('episdes')\n",
    "plt.ylabel('reword')    \n",
    "plt.plot(G)   \n",
    "plt.savefig('PPO_Walker2D_10000episodes.png',dpi=1000)\n",
    "\n",
    "\n",
    "plt.plot(G_mean)\n",
    "plt.ylabel('Average of Returns')\n",
    "plt.xlabel('Number of episodes/10')\n",
    "plt.savefig(\"ReturnsAverage_VS_Episodes PPO_Walker2D_10000\",dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"PPOModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G)\n",
    "plt.ylabel('Returns')\n",
    "plt.xlabel('Number of episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G_mean)\n",
    "plt.ylabel('Average of Returns ')\n",
    "plt.xlabel('Number of episodes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (OpenSimEnv_V2)",
   "language": "python",
   "name": "opensimenv_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
